---
title: 'TOPIC : A PREDICTIVE ANALYSIS OF HOUSING PRICE DETERMINANTS A CASE OF AMES,
  IOWA AND CALIFORNIA USING PREDICTIVE MODELING'
author: "KAGGWA MUSA"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1.0  Introduction
Housing affordability and equitable development have become central concerns in both urban planning and real estate investment. Understanding how structural and spatial features influence home prices is essential for stakeholders, from developers to policymakers as mentioned by M. and Netusil, N.R., (2001). This study compares housing price determinants between Ames, Iowa and California using modern data science techniques, particularly regression modeling and visualization. The Ames Housing and California Housing datasets were selected due to their high quality, public availability, and established use in housing price prediction research. These datasets offer rich, well-structured information, including sale price, living area, bedroom count, and neighborhood, which are critical for developing reliable predictive models. While local housing data might offer geographical relevance, it was either unavailable or lacked sufficient completeness for robust analysis. Additionally, using both Ames (a Midwestern city) and California (a diverse state with varying markets) enables broader model generalizability and comparison across different housing market conditions. This approach ensures the model is trained on comprehensive and diverse inputs that reflect both affordable and high-cost housing scenarios (De Cock, 2011; Kagalwala et al., 2022; Chen & Zhang, 2023).
1.	Problem Statement
In today’s dynamic real estate market, to accurately determine the value of a home is essential for buyers, sellers, agents, Financial Institutions, Government/Policy makers and developers. In Ullah et al (2020), relying solely on intuition or outdated pricing models can lead to poor investment decisions or missed opportunities. 

To address this challenge, this project will lead to the development of a predictive model that will estimate housing sale prices based on key features such as Living Area (LivArea), Total Bedrooms, and Neighborhood characteristics according to Yanming, et al (2022). By use making use of these and other relevant variables, the model will provides reliable price predictions to support informed decision making across the housing markets.
```{r}
2.	Main Objectives
i.	To identify the most influential structural and spatial features affecting housing prices in Ames and California.
ii.	To apply predictive modeling (Linear Regression and XGBoost) for forecasting housing prices.
iii.	To visualize price disparities using Exploratory Data Analysis (EDA), including correlation heatmaps.
iv.	To offer practical recommendations to developers, urban planners, and financial institutions based on model insights.
```
```{r}
3.	Methodology
Data Sources: AmesHousing.csv and CaliforniaHousing.csv, both from Kaggle (Nazari, 2023; Nugent, 2023).
Tools: R programming language, using packages such as ggplot2, xgboost, corrplot, and dplyr.
4.	Data Integration: Merging both datasets on shared fields.
Data integration was done using getting common column names
a.	library(dplyr)
Step 1: View Column Names     
common_cols <- intersect(names(AmesHousing), 
names(Californiahousing))
# Step 1: Get common column names and intersect them using library(dplyr)
names(AmesHousing)
names(Californiahousing)
library(dplyr)
common_cols <- intersect(names(AmesHousing), names(Californiahousing))
# Step 2: Creating subset to common columns
Ames_common <- AmesHousing[, common_cols]
California_common <- Californiahousing[, common_cols]
Step3 #Add sourcename column
Ames_common$sourcename <- "AmesHousing.csv"
California_common$sourcename <- "Californiahousing.csv"
Step 4 #Combine the two
library(dplyr)
combined_data <- bind_rows(Ames_common, California_common)
Interpretation of summary(combined_data)
Neighborhood
The most frequent neighborhoods are San Diego (2132 rows), San Francisco (2116), Long Beach (2094), Santa Ana (2085), Oakland (2079), and Bakersfield (2063).
There are also many other neighborhoods (11,001 rows combined under "Other").
This indicates your dataset covers diverse locations, likely mostly from California cities.
LivArea (Living Area in square feet)
Range: 33 sq ft (smallest) to 5642 sq ft (largest).
Median: 894 sq ft — half the homes are smaller than this, half larger.
Mean: 983.7 sq ft — slightly higher than median, indicating some large houses skew the average upward.
1st Quartile (25%): 447 sq ft
3rd Quartile (75%): 1448 sq ft
Missing values: There are 17,710 NAs (missing values) for LivArea, which is quite large and could affect analysis or modeling.
Total_bedrooms
Range: 0 (possible missing or studio units) to 8 bedrooms.
Median: 3 bedrooms — typical home size.
Mean: 3.411 bedrooms — slightly above median, possibly due to a few large homes.
Quartiles show typical distribution from 2 to 4 bedrooms.
SalePrice
Range: From $12,789 to $755,000.
Median price: $176,100 — half the houses are below, half above this price.
Mean price: $203,616 — mean > median, indicating some high-priced homes pulling the average up.
Quartiles indicate spread of prices: 25% under $122,200, 75% under $257,400.
sourcename
This is a character vector indicating data origin (e.g., "AmesHousing.csv" or "Californiahousing.csv").
Summary here just confirms it’s a character column, no missing values.
```{r}
5.	Data integration and Cleaning: 
Removal of missing values, filtering for consistent Neighborhood and LivArea entries.
Ensure Neighborhood is a factor
combined_data$Neighborhood <- as.factor(combined_data$Neighborhood)
           # Step 1: Remove rows with missing LivArea
combined_data <- combined_data %>% filter(!is.na(LivArea))
# Step 2: Remove rows with unseen Neighborhoods
common_neighs <- intersect(unique(combined_data$Neighborhood), unique(combined_data$Neighborhood))

combined_data <- combined_data %>% filter(Neighborhood %in% common_neighs)
# Step 3: Confirm there are no issues left
summary(combined_data)
colSums(is.na(combined_data))  
# Returned all  0
```
```{r}
6.	Modeling:
Linear Regression to evaluate relationships between variables.
Model Description
This study implemented three distinct supervised learning models—Linear Regression (LM), Random Forest (RF), and Extreme Gradient Boosting (XGBoost)—to predict the target variable using structured tabular data. The modeling workflow was built to accommodate a flexible model_type input, allowing a modular approach to performance benchmarking and interpretability (see Code Snippet X.X in Appendix).
Linear Regression (LM)
The linear regression model was used as a baseline. It assumes a linear relationship between the dependent variable and the set of independent variables. Using the lm() function in R, the model was fit to the training dataset with the formula:
lm(as.formula(paste(target_col, "~ .")), data = train_data)
Extreme Gradient Boosting (XGBoost)
The XGBoost model was trained using the xgboost package. Training data was first converted into xgb.DMatrix objects for efficient computation. The model was configured for regression with the reg: squarederror objective function and trained over 100 boosting rounds. This method is well-suited for tabular data with potential multicollinearity and complex interactions among predictors (Chen & Guestrin, 2016).
xgboost(data = dtrain, nrounds = 100, objective = "reg:squarederror", verbose = 0)
Model Evaluation
For each model, predictions on the test set were generated using the predict() function. The Root Mean Squared Error (RMSE) was calculated to evaluate model performance:
Calculate RMSE (Root Mean Squared Error)
predictions <- predict(model, newdata = test_matrix)
a.	actuals <- test_data$SalePrice
b.	rmse <- sqrt(mean((predictions - actuals)^2))

c.	>print(paste("RMSE: ", rmse))

d.	[1] "RMSE:  107295.419434026"
And an 
Calculated  MAE (Mean Absolute Error)

e.	mae <- mean(abs(predictions - actuals))
	print(paste("MAE: ", mae))

f.	[1] "MAE:  81693.6605846614"

The RMSE metric quantifies the average magnitude of prediction error, allowing for direct comparison across models. Lower RMSE values indicate better model performance on unseen data.
This unified modeling pipeline not only facilitates rapid experimentation across algorithms but also standardizes the evaluation metric for fair model comparison.
RMSE helps measure how accurate your model's predictions are—the lower it is, the better. Using a consistent modeling pipeline with the same steps and same RMSE metric allows you to fairly and efficiently test different models to find the best one.
XGBoost for identifying top predictors of SalePrice..
This model provided interpretability and simplicity, useful for benchmarking more complex algorithms.
Due to Performance-driven approaches and best practices, other methods like Random Forest could be incorporated in the training but given the superior performance and flexibility of XGBoost in handling structured tabular data, we focused our modeling efforts on tuning and evaluating XGBoost. Random Forest would be considered but ultimately excluded, as XGBoost is generally known to outperform it in predictive accuracy, especially on datasets with complex interactions and non-linearities.

           Model Performance Evaluation Using Visualizations
Scatter Plot: Actual vs Predicted Sale Prices
To evaluate the predictive accuracy of the XGBoost model, a scatter plot was created comparing actual house sale prices to the model’s predicted values.
Figure X: Actual vs Predicted Sale Prices
The scatter plot visualizes the relationship between the actual sale prices and the predicted values produced by the model. The red diagonal line represents a perfect prediction line (i.e., where predicted = actual). Most of the points were observed to cluster around this line, suggesting that the model generally makes accurate predictions. However, a few points deviate noticeably, indicating under- or over-estimation in those cases.
Interpretation:
A strong diagonal trend in the scatter plot indicates that the model captures the underlying pattern in the data well.
Points above the red line indicate underestimation by the model.
Points below the red line indicate overestimation.
The scatter suggests moderate variance with some outliers, which is common in real estate data due to property uniqueness and market conditions.

Histogram of Residuals (Prediction Errors)
A histogram of residuals was also plotted to assess the distribution and magnitude of prediction errors.
Figure Y: Distribution of Residuals (Actual - Predicted)
The histogram shows how far off the predictions were from the actual values. Ideally, residuals should be centered around zero and follow a roughly symmetrical, bell-shaped distribution.
Interpretation:
The histogram of residuals was approximately symmetric and centered near zero.
This indicates that the model does not suffer from a strong bias toward over- or under-prediction.
Most errors fall within a reasonable range, confirming that the model has acceptable predictive performance.
However, a few large residuals (extreme over- or under-predictions) suggest that certain properties were particularly difficult to predict, possibly due to outlier characteristics or missing contextual data.
```
```{r}
7.	Visualizations and Analysis
Visual Analysis
The visual evaluation supports the numeric results from model training. The XGBoost model appears to generalize well on the test set, with reasonable prediction accuracy and no major systematic bias. While outliers exist, the majority of predictions fall within a tolerable error margin.

Bar Plot: Average Sale Price by Neighborhood
This grouped bar chart compares average Sale Prices across different Neighborhoods, with bars colored by Sources from"Ames" vs "California intergrated_data
 
The Graph Shows 
Comparison across Neighborhoods
The x-axis lists neighborhoods and Sale Price on the y-axis
Neighborhoods were reordered by descending Sale Price to emphasize relative market values, making it easier for viewers to identify high- and low-value areas at a glance
The y-axis shows the average (or total, depending on how combined_summary is aggregated) Sale Price 
Comparison Between Sources
Each neighborhood has two bars side by side (dodged position):
a.	Steelblue: Ames dataset
b.	Darkorange: California dataset
This lets you visually compare the pricing difference between Ames and California properties within the same neighborhood.
Shows regional disparities in housing values.
Sale prices are consistently higher in certain California neighborhoods.
Source: ggplot2, dataset from keggle.com
Scatter Plot: Sale Price vs. Living Area
 
Positive Linear Relationship
The red line (geom_smooth (method = "lm")) represents a linear regression trend line, fitting the formula y ~ x — that is, SalePrice ~ LivArea.
The upward slope of the line indicates a positive linear relationship: as Living Area an increase, Sale Price tends to increase as well.
A linear upward trend indicates larger homes generally have higher prices.
Outliers reflect other contributing factors like amenities and location. Summary Sentence for a Report:
However, several positive outliers were observed, as representing homes that sold well above the typical price for their size likely due to unmodeled premium features or location advantages. These can be investigated for validity and handle them precisely using robust modeling techniques to minimize bias.
Point Distribution (alpha = 0.5)
Each point represents a property.
The alpha = 0.5 makes overlapping points semi-transparent, helping reveal density patterns.
You'll likely notice more clustering in certain ranges (e.g., 1,000–2,500 sq ft), which may suggest typical home sizes that 3-4 bedRooms
Box Plot: Sale Price by Bedroom Count
 
It shows that Median sale prices increase with more bedrooms but exhibit diminishing returns beyond 
Shows wide variability for 3- and 4-bedroom homes.

XGBoost Feature Importance Plot
Top features: 
c.	 
Interpretation:
There is a positive nonlinear relationship: homes with more bedrooms generally sell for more, but with diminishing returns after 4 bedrooms.
In both datasets, 3–4 bedrooms appear as the sweet spot for value, offering enough space without incurring inefficient price jumps.
According to the correlation heatmap, we can observe linear associations between features and the target variable, which may suggest their potential relevance in linear models such as linear regression or regularized regression models and this was also hint on by Chen & Guestrin (2016).
Correlation Heatmap
Shows correlation between numeric variables such as SalePrice, LivArea, and Total_bedrooms.
d.	 
Strong positive correlations support variable selection for modeling.
This was achieved through the use of Predicted data, a dataset integrated from both AmesHousing.csv and California.csv, followed by correlation analysis using tools recommended by Wei & Simko (2017).
```
```{r}
8.	Broader Impact in Developing Economies
a.	The results highlight the relevance of physical size, bedroom count, and neighborhood features. In developing nations according to  Saks, R.E. (2005),  these insights can guide equitable housing policy to support growing urban populations, reduce overcrowding, and promote gender and education equity.

```{r} 
10.	Conclusions and Recommendations

10.1.	Conclusion: 
Structural features like living area and bedroom count, alongside spatial factors like neighborhood, are critical to understanding housing price disparities as recorde from the Department of Housing and Urban Development's (HUD) and Market Analysis as of January 1, 2021.These findings are consistent across Ames and California, though regional effects (e.g., market saturation in California) amplify disparities.
```

```{r}
10.2.	Recommendations:
Developers: Should prioritize modular 3+ bedroom units.
Planners should often focus on infrastructure development in low-price neighborhoods for several interconnected social, economic, and strategic reasons to reduce on inequality and improves social life quality.
Financial institutions should incorporate structural predictors into appraisal models, as structural features such as size, condition, and building materials  as they are often more strongly correlated with sale price than location or amenities. Including these predictors enhances the accuracy and fairness of property valuations.
Government / Policymakers
Incentivize affordable housing in underdeveloped neighborhoods through tax credits, zoning reforms, or public-private partnerships.
Enforce data-driven appraisal guidelines that incorporate structural and locational predictors to reduce valuation bias.
Support neighborhood revitalization programs that combine housing, employment, and community investments.
Investors
Should target undervalued neighborhoods with strong potential for appreciation based on structural predictors (e.g., large living areas, newer builds) and improving infrastructure.
Diversify portfolios by including mid-range properties in emerging markets, where demand and price growth are accelerating

```

```{r}
11.	References
i.	De Cock, D. (2011). Ames, Iowa: Alternative to the Boston housing data as an 	end of 	semester regression project. Journal of Statistics Education, 19(3). 	https://doi.org/10.1080/10691898.2011.11889627
ii.	Kagalwala, A., Patel, R., & Shah, M. (2022). Predictive modeling and feature 	importance in real estate valuation using XGBoost and Random Forest. 	International Journal of Advanced Computer Science and Applications, 	13(6), 	389–395. https://doi.org/10.14569/IJACSA.2022.0130649
iii.	Chen, Y., & Zhang, L. (2023). A comparative study of machine learning models 	for 	housing price prediction. Journal of Data Science and Analytics, 	5(1), 42–55.
iv.	Mamun, M. A., & Ullah, I. (2020). COVID-19 suicides in Pakistan, dying off not 	COVID-19 	fear but poverty?–The forthcoming economic challenges 	for a 	developing 	country. Brain, 	behavior, and immunity, 87, 	163-166.
v.	Chen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting system. 	Proceedings 	of the 	22nd ACM SIGKDD, 785–794.
vi.	Yang, Yanming, et al. "Predictive models in software engineering: Challenges 	and 	opportunities." ACM Transactions on Software Engineering and 	Methodology 	(TOSEM) 31.3 (2022): 1-72.
vii.	Kain, J. F., & Quigley, J. M. (1970). Measuring the value of housing quality. 	JASA, 	65(330), 	532–548.
viii.	Malpezzi, S. (2003). Hedonic pricing models: A selective and applied review. In 	Housing 	Economics and Public Policy, 67–89.
ix.	Rosen, S. (1974). Hedonic prices and implicit markets. Journal of Political 	Economy, 	82(1), 34–	55.
x.	Wei, T., & Simko, V. (2017). R package "corrplot": Visualization of a correlation 	matrix. 	R 	package version 0.84.
xi.	Wickham, H. (2016). ggplot2: Elegant Graphics for Data Analysis. Springer.

xii.	Nazari, H. (2023). Ames Housing Dataset. Kaggle. 	[https://www.kaggle.com/code/hnazari8665/ames-housing-dataset]
xiii.	Nugent, C. (2023). California Housing Prices Dataset. Kaggle. 	[https://www.kaggle.com/datasets/camnugent/california-housing-prices]
```
